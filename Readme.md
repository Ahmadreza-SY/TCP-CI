# Code Feature Extraction

## Introduction

This project aims to extract and compute test case features from a software repository. It analyzes the source code both statically and based on its version control history and creates dependency graphs containing the relationships between files/functions and their association weights. The goal of collecting these features is for solving the Test Case Prioritization (TCP) problem.

## Environment Setup
### Python Environment
The tool is tested on Python 3.7+. The tool's Python dependencies can be installed via the following command:
```bash
pip install -r requirements.txt
```
### Understand
Understand is a code analysis enterprise software with a wide variety of [supported languages](https://support.scitools.com/t/supported-languages/153) which provides static dependencies available in a source code between files, functions, classes, etc. For more details on this software, visit [this link](https://scitools.com/features). In this project, we utilize Understand to create static dependency graphs to collect a part of our features. 

In this section, we will explain how to install and set up Understand to obtain a file with `.und` format which is the output of Understand's analysis. Note that this project needs Understand's database for extracting features and won't work without it.

#### Installing Understand's CLI
You can download the latest stable version of Understand from [this link](https://licensing.scitools.com/download). To run this project, you need to add the `und` command to your PATH environment variable so the `und` command is recognized in the shell. `und` is located in the `bin` directory of Understand's software.

```bash
export PATH="$PATH:/path/to/understand/scitools/bin/linux64"
```

Finally, run the following command to make sure `und` is successfully installed:

```bash
$ und version
(Build 1029)
```

#### Note
This tool has been tested on Build 1029 of Understand on Linux (specifically Ubuntu). It may require minor compatibility changes if it is used on other Understand builds or other operating systems.

#### Adding Understand Python Package/Library
Unlike typical projects, Understand does not provide its Python library in the well-known pip package installer, and you need to manually add the package to your Python environment. The instructions for adding the package are explained in [this link](https://support.scitools.com/t/getting-started-with-the-python-api/51).

## Usage Instructions
After setting up the environment, you're ready to start using the tool.

This project consists of multiple sub-commands, and each peforms different tasks. Each sub-commands is explained in the following sections.

### The `tr_torrent` Sub-command

This sub-command processes maven build logs that were downloaded by [TravisTorrent](https://github.com/TestRoots/travistorrent-tools) and creates the CI data required by the `dataset` sub-command. This sub-command requires the `repo-data-travis.json` file and the build logs generated by TravisTorrent for each project. The sub-command arguments are shown in the following table.

Argument Name | Description
--- | --- | ---
-r REPO, --repo REPO | The login and name of the repo seperated by @ (e.g., neuland@jade4j)
-i INPUT_PATH, --input-path INPUT_PATH | Specifies the directory to of travis torrent raw data (builds info and logs).
-o OUTPUT_PATH, --output-path OUTPUT_PATH | Specifies the directory to save resulting data.

An example of using the `tr_torrent` sub-command:
```
python main.py tr_torrent -i ../travistorrent-tools -o ../tr-torrent -r neuland@jade4j
```

### The `dataset` Sub-command
This sub-command collects and creates a training dataset including all test case features for each failed CI build (cycle). It requires two main sources of data: source code and CI data. The source code can be either in form of a GitHub slug or a path to the source code. For CI data, a path should be provided which includes a repository folder containing two csv files with a specific format (the file format will be discussed later). The sub-command arguments are shown in the following table.

Argument Name | Description
--- | --- | ---
-p PROJECT_PATH, --project-path PROJECT_PATH | Project's source code git repository path.
-s PROJECT_SLUG, --project-slug PROJECT_SLUG | The project's GitHub slug, e.g., apache/commons.
-c CI_DATA_PATH, --ci-data-path CI_DATA_PATH | Path to CI datasource root directory.
-o OUTPUT_PATH, --output-path OUTPUT_PATH | Specifies the directory to save resulting datasets.

An example of using the `dataset` sub-command:
```
python main.py dataset -s neuland/jade4j -c ../rtp-torrent -o ./datasets/neuland@jade4j
```

#### Notes
At least one of the `--project-path` or `--project-slug` arguments should be provided since the tool requires the source code for analysis.

#### The CI Data File Format
The CI data files for each project should be under a folder named with its repository name (login and name of the repo seperated by @, e.g., neuland@jade4j). The repository folder should contain two files with names `<repo-name>-full.csv` and `<repo-name>-builds.csv` (e.g., `neuland@jade4j-full.csv`, `neuland@jade4j-builds.csv`). The `<repo-name>-full.csv` file should contain test case execution records for all jobs across all builds, which includes the following columns:
- `travisBuildId`: Id of the Travis CI build.
- `travisJobId`: Id of the Travis CI job.
- `testName`: The class name of the test case including its package name (e.g., `com.example.Test`).
- `duration`: The duration of the test case execution in seconds.
- `count`: The number of test methods executed in the test class.
- `failures`: The number of failures occured in the execution of the test class.
- `errors`: The number of errors occured in the execution of the test class.
- `skipped`: The number of skipped test methods in the execution of test class.
**Notes:** The full test class name is required for matching it with its corresponding source file. Also, the last four columns are based on the data available in Maven logs. `failures` refer to test cases failures due to wrong outputs or assertion errors, whereas `errors` refer to unexpected behaviours such as runtime exceptions.

The `<repo-name>-builds.csv` file should contain build information, which including the following columns:
tr_build_id,git_all_built_commits,gh_build_started_at,jobs
- `tr_build_id`: Id of the Travis CI build.
- `git_all_built_commits`: Full hashes of all git commits that were built in the build which are separated by the `#` character.
- `gh_build_started_at`: The starting date and time of the build (e.g., `2020-01-15 17:56:24 UTC`).
- `jobs`: A json list of all Travis CI job ids that were executed in the build (e.g., `"[620564834, 620564835]"`).

### The `learn` Sub-command
This sub-command performs learning (training and evaluation) experiments on collected features using RankLib. Since it requires test case features as input, the `dataset` sub-command should be successfully executed before executing this sub-command. The sub-command arguments are shown in the following table.

Argument Name | Description
--- | --- | ---
-o OUTPUT_PATH, --output-path OUTPUT_PATH | Specifies the directory to load required data and save results.
-t TEST_COUNT, --test-count TEST_COUNT | Specifies the number of recent builds to test the trained models on.

An example of using the `learn` sub-command:
```
python main.py learn -o ./datasets/neuland@jade4j -t 50
```

### The `decay_test` Sub-command
This sub-command performs TCP model decay experiments on trained models (for more details about this experiment, refer to the paper). Since it reuses the previously trained models, the `learn` sub-command should be successfully executed before executing this sub-command. The sub-command arguments are shown in the following table.

Argument Name | Description
--- | --- | ---
-o OUTPUT_PATH, --output-path OUTPUT_PATH | Specifies the directory to load required data and save results.
-p PROJECT_PATH, --project-path PROJECT_PATH | Project's source code git repository path.

An example of using the `decay_test` sub-command:
```
python main.py decay_test -o ./datasets/neuland@jade4j -p ./datasets/neuland@jade4j/jade4j
```


### The `results` Sub-command
This sub-command analyzes the results from above experiments across all projects (subjects) and generates tables. It requires the execution of all other sub-commands across projects since it analyzes all results to answer the paper's research questions. The sub-command arguments are shown in the following table.

Argument Name | Description
--- | --- | ---
-d DATA_PATH, --data-path DATA_PATH | Path to the root folder of all datasets.
-o OUTPUT_PATH, --output-path OUTPUT_PATH | Specifies the directory to save resulting tables.

An example of using the `results` sub-command:
```
python main.py results -d ./datasets -o ./results
```

## Outputs

In this section, we will describe output files that each sub-command creates.

### `dataset` Output Files

The `dataset` sub-command creates the following files as its outputs: 
- `dataset.csv`: Contains all test case features across failed CI builds.
- `exe.csv`: Contains the execution history of the test cases that is used to create the dataset.
- `entity_change_history.csv`: Contains the change history of source code entities based on all Git commits.
- `id_map.csv`: Contains a map that relates a source code file relative path to a unique integer identifier.
- `builds.csv`: Contains information of CI builds.
- `contributors.csv`: Contains information of contributors the project.

#### The `analysis` Folder
In addtion to above csv files, the `dataset` sub-command creates a directory named `analysis` which includes a number of sub-folders for each build which are named with build ids. Each sub-folder contains a `metadata.csv` file, and each sub-folder corresponding to failed builds contain a `tar.csv` and a `dep.csv` file. The `metadata.csv` file represents static features for each entity (file) captured by Understand for each build. Please visit [this link](https://support.scitools.com/t/what-metrics-does-undertand-have/66) for descriptions of all Understand metrics.

The `dep.csv` file represents static and historical dependencies between the entities of the system under test (SUT) which can be found in `metadata.csv`. Static dependencies are extracted from Understand's database, and historical dependencies are represented by association weights for each static dependency which can be found in the `weights` column. The association weights are extracted using co-changes in git commits and the Apriori algorithm. For each dependency, the association weight is either a single zero or four real numbers. A single zero means although Understand detected a static relation, there is no historical dependency between these two entities. In other words, among all analyzed commits, there is no commit in which both entities have changed. On the other hand, the four real numbers represent `support`, `forward_confidence`, `backward_confidence`, and `lift`. Note that assuming A has a dependency to B, `forward_confidence` is the confidence for A given B (A|B) and `backward_confidence` is the confidence for B given A (B|A). For details about these association metrics, visit [this link](https://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html). Additionally, the `tar.csv` file represents the dependencies between test entities and entities of the SUT. This file is similar to `dep.csv`.

#### Notes
- The `test_result` column in the `exe.csv` file can have the 4 following values: 0 for success/passed, 1 for failure due to exception, 2 for failure due to assertion, and 3 for failure due to an unknown reason.

### `learn` Output Files
The `learn` sub-command creates a folder named `tsp_accuracy_results` which includes sub-folders for each learning experiment (e.g. `full` for learning using all features). Under each learning experiment sub-folder, there is a `results.csv` file (including the accuracies across builds), and there are a number of folders named with failed build ids that were used for evaluation, and each folder contains a trained model (`model.txt`), its stats (`feature_stats.csv`), an evaluation test suite (`test.txt`), and its prioritized prediction (`pred.txt`).
#### Notes
- All `.txt` files are in the format required by RankLib.
- The `W-` and `wo-` prefixes stand for with and without, respectively. The feature group after these prefixes indicates the features that the models were trained on.
- The `-outliers` suffix refers to the experiments in which outlier (frequent-failing) test cases were removed. For more details about outlier tests, please refer to the paper.
- For both `full` and `full-outliers` experiments, there is also two `heurisitc_<metric>_results.csv` (`<metric>` can be `apfd` or `apfdc`) files which include TCP evaluation results when using heursitics. For more details about our heurisitc experiments, please refer to the paper.

### `decay` Output Files
The `decay` sub-command creates a folder named `decay_datasets` which includes sub-folders named with failed build ids that were used for evaluation, and each sub-folder contains a `dataset.csv` file (the dataset is used to test its subsequent builds), a `test.txt` and a `pred.txt` file which include all test builds and prioritization predictions, respectively, and a `results.csv` file which includes accuracies of the predictions. For more details about the decay experiment, please refer to the paper.

### `results` Output Files
The `results` sub-command creates a `subject_stats.csv` file which includes the statistics of the projects (subjects) that were given as input. This sub-command also create three sub-folders: `RQ1`, `RQ2`, and `RQ3`. Each of these sub-folders includes results of the analysis done to answer the paper's research questions based on experiment outcomes. For more details about the research question, please refer to the paper.
